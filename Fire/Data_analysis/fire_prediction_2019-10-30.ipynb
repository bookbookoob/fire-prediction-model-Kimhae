{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.formula.api import glm\n",
    "from statsmodels.genmod.families.family import Binomial\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('train_PCA_fill_ar.csv')\n",
    "val = pd.read_csv('valid_PCA_fill_ar.csv')\n",
    "test = pd.read_csv('test_PCA_fill_ar.csv')\n",
    "sub = pd.read_csv('../Data/PJT002_submission.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 필요한 열만 가져오기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['id', 'dt_of_fr', 'bldng_cnt', 'bldng_ar', 'ttl_ar', 'lnd_ar'\n",
    "#            , 'jmk', 'fr_sttn_dstnc', 'fr_wthr_fclt_dstnc'\n",
    "#            , 'mlt_us_yn', 'cctv_dstnc', 'cctv_in_100m'\n",
    "#            , 'fr_wthr_fclt_in_100m', 'tbc_rtl_str_dstnc', 'sft_emrgnc_bll_dstnc'\n",
    "#            , 'ahsm_dstnc', 'no_tbc_zn_dstnc', 'bldng_cnt_in_50m', 'fr_yn'\n",
    "#           ]\n",
    "# columns2 = ['id', 'dt_of_fr', 'bldng_cnt', 'bldng_ar', 'ttl_ar', 'lnd_ar'\n",
    "#            , 'jmk', 'fr_sttn_dstnc', 'fr_wthr_fclt_dstnc'\n",
    "#            , 'mlt_us_yn', 'cctv_dstnc', 'cctv_in_100m'\n",
    "#            , 'fr_wthr_fclt_in_100m', 'tbc_rtl_str_dstnc', 'sft_emrgnc_bll_dstnc'\n",
    "#            , 'ahsm_dstnc', 'no_tbc_zn_dstnc', 'bldng_cnt_in_50m'\n",
    "#           ]\n",
    "\n",
    "# train_pp = train[columns]\n",
    "# test_pp = test[columns2]\n",
    "# val_pp = val[columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NULL CHECK\n",
    "null 없는 애들로만 가져와서 null이 없음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 59191 entries, 0 to 59190\n",
      "Data columns (total 93 columns):\n",
      "address                  59191 non-null object\n",
      "address1                 59191 non-null object\n",
      "address2                 59191 non-null object\n",
      "ahsm_dstnc               59191 non-null int64\n",
      "bldng_ar                 0 non-null float64\n",
      "bldng_cnt                59191 non-null int64\n",
      "bldng_cnt_in_50m         59191 non-null int64\n",
      "cctv_dstnc               59191 non-null int64\n",
      "cctv_in_100m             59191 non-null int64\n",
      "day                      59191 non-null int64\n",
      "dayofweek                59191 non-null object\n",
      "fire_yes                 59191 non-null float64\n",
      "fr_mn_cnt                59191 non-null float64\n",
      "fr_sttn_dstnc            59191 non-null int64\n",
      "fr_wthr_fclt_dstnc       59191 non-null int64\n",
      "fr_wthr_fclt_in_100m     59191 non-null int64\n",
      "hm_cnt                   59191 non-null float64\n",
      "hmdt                     59191 non-null float64\n",
      "hour                     59191 non-null int64\n",
      "id                       59191 non-null int64\n",
      "jmk                      59191 non-null object\n",
      "lnd_ar                   0 non-null float64\n",
      "minute                   59191 non-null int64\n",
      "mlt_us_yn(encode)        59191 non-null bool\n",
      "month                    59191 non-null int64\n",
      "no_tbc_zn_dstnc          59191 non-null int64\n",
      "second                   59191 non-null int64\n",
      "sft_emrgnc_bll_dstnc     59191 non-null int64\n",
      "tag                      59191 non-null object\n",
      "tbc_rtl_str_dstnc        59191 non-null int64\n",
      "tmprtr                   59191 non-null float64\n",
      "ttl_ar                   0 non-null float64\n",
      "wnd_drctn                59191 non-null float64\n",
      "wnd_spd                  59191 non-null float64\n",
      "year                     59191 non-null int64\n",
      "year-month               59191 non-null object\n",
      "year-month-day           59191 non-null object\n",
      "cluster                  59191 non-null int64\n",
      "Comp1                    59191 non-null float64\n",
      "Comp2                    59191 non-null float64\n",
      "jmk_주                    59191 non-null bool\n",
      "jmk_잡                    59191 non-null bool\n",
      "jmk_종                    59191 non-null bool\n",
      "jmk_차                    59191 non-null bool\n",
      "jmk_답                    59191 non-null bool\n",
      "jmk_장                    59191 non-null bool\n",
      "jmk_양                    59191 non-null bool\n",
      "jmk_전                    59191 non-null bool\n",
      "jmk_창                    59191 non-null bool\n",
      "jmk_학                    59191 non-null bool\n",
      "jmk_목                    59191 non-null bool\n",
      "jmk_도                    59191 non-null bool\n",
      "jmk_임                    59191 non-null bool\n",
      "jmk_철                    59191 non-null bool\n",
      "jmk_유                    59191 non-null bool\n",
      "jmk_천                    59191 non-null bool\n",
      "jmk_구                    59191 non-null bool\n",
      "jmk_체                    59191 non-null bool\n",
      "jmk_과                    59191 non-null bool\n",
      "jmk_묘                    59191 non-null bool\n",
      "jmk_공                    59191 non-null bool\n",
      "jmk_원                    59191 non-null bool\n",
      "jmk_사                    59191 non-null bool\n",
      "jmk_제                    59191 non-null bool\n",
      "jmk_수                    59191 non-null bool\n",
      "lnd_us_sttn_nm(clean)    59191 non-null object\n",
      "lnd_us_sttn_nm_주         59191 non-null bool\n",
      "lnd_us_sttn_nm_잡         59191 non-null bool\n",
      "lnd_us_sttn_nm_종         59191 non-null bool\n",
      "lnd_us_sttn_nm_차         59191 non-null bool\n",
      "lnd_us_sttn_nm_답         59191 non-null bool\n",
      "lnd_us_sttn_nm_장         59191 non-null bool\n",
      "lnd_us_sttn_nm_양         59191 non-null bool\n",
      "lnd_us_sttn_nm_전         59191 non-null bool\n",
      "lnd_us_sttn_nm_창         59191 non-null bool\n",
      "lnd_us_sttn_nm_학         59191 non-null bool\n",
      "lnd_us_sttn_nm_목         59191 non-null bool\n",
      "lnd_us_sttn_nm_도         59191 non-null bool\n",
      "lnd_us_sttn_nm_임         59191 non-null bool\n",
      "lnd_us_sttn_nm_철         59191 non-null bool\n",
      "lnd_us_sttn_nm_유         59191 non-null bool\n",
      "lnd_us_sttn_nm_천         59191 non-null bool\n",
      "lnd_us_sttn_nm_구         59191 non-null bool\n",
      "lnd_us_sttn_nm_체         59191 non-null bool\n",
      "lnd_us_sttn_nm_과         59191 non-null bool\n",
      "lnd_us_sttn_nm_묘         59191 non-null bool\n",
      "lnd_us_sttn_nm_공         59191 non-null bool\n",
      "lnd_us_sttn_nm_원         59191 non-null bool\n",
      "lnd_us_sttn_nm_사         59191 non-null bool\n",
      "lnd_us_sttn_nm_제         59191 non-null bool\n",
      "lnd_us_sttn_nm_수         59191 non-null bool\n",
      "ttl_dwn_flr              59191 non-null float64\n",
      "ttl_grnd_flr             59191 non-null float64\n",
      "dtypes: bool(51), float64(14), int64(19), object(9)\n",
      "memory usage: 21.8+ MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = train[train['address1'] != '창원시']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        True\n",
       "1        True\n",
       "2        True\n",
       "3        True\n",
       "4        True\n",
       "         ... \n",
       "59186    True\n",
       "59187    True\n",
       "59188    True\n",
       "59189    True\n",
       "59190    True\n",
       "Name: address1, Length: 50749, dtype: bool"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train['address1'] != '창원시'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAKING DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose independent variables(Xs) which are useful!\n",
    "# 트레이닝에 사용할 변수 목록을 적어주세요.\n",
    "independents = [\n",
    "     'bldng_cnt', 'bldng_ar', 'ttl_ar', 'lnd_ar', 'ttl_dwn_flr', 'ttl_grnd_flr',\n",
    "#       'jmk',\n",
    "#    'address1', \n",
    "    'fr_wthr_fclt_in_100m', 'mlt_us_yn(encode)',\n",
    "       'tbc_rtl_str_dstnc',\n",
    "    \n",
    "#     'fr_sttn_dstnc', 'fr_wthr_fclt_dstnc',  'cctv_dstnc',\n",
    "#        'cctv_in_100m', \n",
    "#        'sft_emrgnc_bll_dstnc', 'ahsm_dstnc', 'no_tbc_zn_dstnc',\n",
    "#        'bldng_cnt_in_50m',  \n",
    "    'month',\n",
    "#         'year-month'\n",
    "] \n",
    "# 독립변수\n",
    "dependent = ['fire_yes'] # 종속변수"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 적어준 변수 목록을 사용해 데이터를 트레이닝에 맞는 포맷으로 변경합니다.\n",
    "train_X = train[independents]\n",
    "train_y = train[dependent]\n",
    "\n",
    "test_X = test[independents]\n",
    "val_X = val[independents]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL FITTING"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 선형 회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input contains NaN, infinity or a value too large for dtype('float64').",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-db5824e34edf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLinearRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# 대문자는 매트릭스, 소문자는 벡터\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/linear_model/base.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    461\u001b[0m         \u001b[0mn_jobs_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_jobs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'],\n\u001b[0;32m--> 463\u001b[0;31m                          y_numeric=True, multi_output=True)\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0msample_weight\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0matleast_1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    717\u001b[0m                     \u001b[0mensure_min_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mensure_min_features\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    718\u001b[0m                     \u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwarn_on_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 719\u001b[0;31m                     estimator=estimator)\n\u001b[0m\u001b[1;32m    720\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mmulti_output\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    721\u001b[0m         y = check_array(y, 'csr', force_all_finite=True, ensure_2d=False,\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, warn_on_dtype, estimator)\u001b[0m\n\u001b[1;32m    540\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mforce_all_finite\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    541\u001b[0m             _assert_all_finite(array,\n\u001b[0;32m--> 542\u001b[0;31m                                allow_nan=force_all_finite == 'allow-nan')\n\u001b[0m\u001b[1;32m    543\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mensure_min_samples\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_assert_all_finite\u001b[0;34m(X, allow_nan)\u001b[0m\n\u001b[1;32m     54\u001b[0m                 not allow_nan and not np.isfinite(X).all()):\n\u001b[1;32m     55\u001b[0m             \u001b[0mtype_err\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'infinity'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mallow_nan\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m'NaN, infinity'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 56\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg_err\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtype_err\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     57\u001b[0m     \u001b[0;31m# for object dtype data, we only check for NaNs (GH-13254)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'object'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mallow_nan\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Input contains NaN, infinity or a value too large for dtype('float64')."
     ]
    }
   ],
   "source": [
    "reg = LinearRegression().fit(train_X, train_y)\n",
    "# 대문자는 매트릭스, 소문자는 벡터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-55146c60f40a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# x 계수\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# x 계수의 의미를 해석하세요.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcoef_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# 피클래스가 한 계단 올라갈 때마다 죽음에 0.19배 가까워짐\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "# x 계수\n",
    "# x 계수의 의미를 해석하세요.\n",
    "reg.coef_\n",
    "# 피클래스가 한 계단 올라갈 때마다 죽음에 0.19배 가까워짐"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-173e408d62c3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 절편\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintercept_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "# 절편\n",
    "reg.intercept_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'reg' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-ff4af7e56716>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mreg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_X\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'reg' is not defined"
     ]
    }
   ],
   "source": [
    "reg.score(train_X, train_y, sample_weight=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 일반화 선형모형"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>Generalized Linear Model Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>       <td>fire_yes</td>     <th>  No. Observations:  </th>  <td> 50749</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                  <td>GLM</td>       <th>  Df Residuals:      </th>  <td> 50745</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model Family:</th>        <td>Binomial</td>     <th>  Df Model:          </th>  <td>     3</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Link Function:</th>         <td>logit</td>      <th>  Scale:             </th> <td>  1.0000</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>                <td>IRLS</td>       <th>  Log-Likelihood:    </th> <td> -21121.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>            <td>Wed, 13 Nov 2019</td> <th>  Deviance:          </th> <td>  42242.</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                <td>12:30:25</td>     <th>  Pearson chi2:      </th> <td>5.12e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Iterations:</th>          <td>5</td>        <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>     <td>nonrobust</td>    <th>                     </th>     <td> </td>   \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "            <td></td>              <th>coef</th>     <th>std err</th>      <th>z</th>      <th>P>|z|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th>            <td>   -1.3375</td> <td>    0.030</td> <td>  -43.861</td> <td> 0.000</td> <td>   -1.397</td> <td>   -1.278</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>fr_wthr_fclt_in_100m</th> <td>    0.1399</td> <td>    0.018</td> <td>    7.935</td> <td> 0.000</td> <td>    0.105</td> <td>    0.174</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>tbc_rtl_str_dstnc</th>    <td>-9.892e-05</td> <td> 4.52e-06</td> <td>  -21.866</td> <td> 0.000</td> <td>   -0.000</td> <td>-9.01e-05</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month</th>                <td>   -0.0190</td> <td>    0.004</td> <td>   -5.299</td> <td> 0.000</td> <td>   -0.026</td> <td>   -0.012</td>\n",
       "</tr>\n",
       "</table>"
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                 Generalized Linear Model Regression Results                  \n",
       "==============================================================================\n",
       "Dep. Variable:               fire_yes   No. Observations:                50749\n",
       "Model:                            GLM   Df Residuals:                    50745\n",
       "Model Family:                Binomial   Df Model:                            3\n",
       "Link Function:                  logit   Scale:                          1.0000\n",
       "Method:                          IRLS   Log-Likelihood:                -21121.\n",
       "Date:                Wed, 13 Nov 2019   Deviance:                       42242.\n",
       "Time:                        12:30:25   Pearson chi2:                 5.12e+04\n",
       "No. Iterations:                     5                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "========================================================================================\n",
       "                           coef    std err          z      P>|z|      [0.025      0.975]\n",
       "----------------------------------------------------------------------------------------\n",
       "Intercept               -1.3375      0.030    -43.861      0.000      -1.397      -1.278\n",
       "fr_wthr_fclt_in_100m     0.1399      0.018      7.935      0.000       0.105       0.174\n",
       "tbc_rtl_str_dstnc    -9.892e-05   4.52e-06    -21.866      0.000      -0.000   -9.01e-05\n",
       "month                   -0.0190      0.004     -5.299      0.000      -0.026      -0.012\n",
       "========================================================================================\n",
       "\"\"\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = glm('fire_yes ~ fr_wthr_fclt_in_100m + tbc_rtl_str_dstnc + month', train, family=Binomial()).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-f40d5d0cf3ec>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mglm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'fire_yes ~ C(address1)*C(jmk)'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mBinomial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, start_params, maxiter, method, tol, scale, cov_type, cov_kwds, use_t, full_output, disp, max_start_irls, **kwargs)\u001b[0m\n\u001b[1;32m   1026\u001b[0m             return self._fit_irls(start_params=start_params, maxiter=maxiter,\n\u001b[1;32m   1027\u001b[0m                                   \u001b[0mtol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtol\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcov_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcov_type\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1028\u001b[0;31m                                   cov_kwds=cov_kwds, use_t=use_t, **kwargs)\n\u001b[0m\u001b[1;32m   1029\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1030\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_optim_hessian\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'optim_hessian'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/statsmodels/genmod/generalized_linear_model.py\u001b[0m in \u001b[0;36m_fit_irls\u001b[0;34m(self, start_params, maxiter, tol, scale, cov_type, cov_kwds, use_t, **kwargs)\u001b[0m\n\u001b[1;32m   1165\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_endog\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1166\u001b[0m                                             check_weights=True)\n\u001b[0;32m-> 1167\u001b[0;31m             \u001b[0mwls_results\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwls_mod\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwls_method\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1168\u001b[0m             \u001b[0mlin_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwls_results\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1169\u001b[0m             \u001b[0mlin_pred\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_offset_exposure\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/statsmodels/regression/_tools.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, method)\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    101\u001b[0m             params, _, _, _ = np.linalg.lstsq(self.wexog, self.wendog,\n\u001b[0;32m--> 102\u001b[0;31m                                               rcond=-1)\n\u001b[0m\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m         \u001b[0mfitted_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/numpy/linalg/linalg.py\u001b[0m in \u001b[0;36mlstsq\u001b[0;34m(a, b, rcond)\u001b[0m\n\u001b[1;32m   2266\u001b[0m         \u001b[0;31m# lapack can't handle n_rhs = 0 - so allocate the array one larger in that axis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2267\u001b[0m         \u001b[0mb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_rhs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2268\u001b[0;31m     \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrank\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgufunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrcond\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msignature\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mextobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mextobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2269\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mm\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2270\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m...\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "res = glm('fire_yes ~ C(address1)*C(jmk)', train, family=Binomial()).fit()\n",
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 최소제곱추정 선형회귀"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res = ols('fire_yes ~ month', train).fit()\n",
    "# res.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table class=\"simpletable\">\n",
       "<caption>OLS Regression Results</caption>\n",
       "<tr>\n",
       "  <th>Dep. Variable:</th>        <td>fire_yes</td>     <th>  R-squared:         </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Model:</th>                   <td>OLS</td>       <th>  Adj. R-squared:    </th> <td>   0.000</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Method:</th>             <td>Least Squares</td>  <th>  F-statistic:       </th> <td>   25.33</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Date:</th>             <td>Wed, 13 Nov 2019</td> <th>  Prob (F-statistic):</th> <td>4.86e-07</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Time:</th>                 <td>12:33:18</td>     <th>  Log-Likelihood:    </th> <td> -19846.</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>No. Observations:</th>      <td> 50749</td>      <th>  AIC:               </th> <td>3.970e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Residuals:</th>          <td> 50747</td>      <th>  BIC:               </th> <td>3.971e+04</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Df Model:</th>              <td>     1</td>      <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Covariance Type:</th>      <td>nonrobust</td>    <th>                     </th>     <td> </td>    \n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "      <td></td>         <th>coef</th>     <th>std err</th>      <th>t</th>      <th>P>|t|</th>  <th>[0.025</th>    <th>0.975]</th>  \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Intercept</th> <td>    0.1656</td> <td>    0.003</td> <td>   49.496</td> <td> 0.000</td> <td>    0.159</td> <td>    0.172</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>month</th>     <td>   -0.0023</td> <td>    0.000</td> <td>   -5.032</td> <td> 0.000</td> <td>   -0.003</td> <td>   -0.001</td>\n",
       "</tr>\n",
       "</table>\n",
       "<table class=\"simpletable\">\n",
       "<tr>\n",
       "  <th>Omnibus:</th>       <td>16624.062</td> <th>  Durbin-Watson:     </th> <td>   1.926</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Prob(Omnibus):</th>  <td> 0.000</td>   <th>  Jarque-Bera (JB):  </th> <td>39076.087</td>\n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Skew:</th>           <td> 1.950</td>   <th>  Prob(JB):          </th> <td>    0.00</td> \n",
       "</tr>\n",
       "<tr>\n",
       "  <th>Kurtosis:</th>       <td> 4.807</td>   <th>  Cond. No.          </th> <td>    15.7</td> \n",
       "</tr>\n",
       "</table><br/><br/>Warnings:<br/>[1] Standard Errors assume that the covariance matrix of the errors is correctly specified."
      ],
      "text/plain": [
       "<class 'statsmodels.iolib.summary.Summary'>\n",
       "\"\"\"\n",
       "                            OLS Regression Results                            \n",
       "==============================================================================\n",
       "Dep. Variable:               fire_yes   R-squared:                       0.000\n",
       "Model:                            OLS   Adj. R-squared:                  0.000\n",
       "Method:                 Least Squares   F-statistic:                     25.33\n",
       "Date:                Wed, 13 Nov 2019   Prob (F-statistic):           4.86e-07\n",
       "Time:                        12:33:18   Log-Likelihood:                -19846.\n",
       "No. Observations:               50749   AIC:                         3.970e+04\n",
       "Df Residuals:                   50747   BIC:                         3.971e+04\n",
       "Df Model:                           1                                         \n",
       "Covariance Type:            nonrobust                                         \n",
       "==============================================================================\n",
       "                 coef    std err          t      P>|t|      [0.025      0.975]\n",
       "------------------------------------------------------------------------------\n",
       "Intercept      0.1656      0.003     49.496      0.000       0.159       0.172\n",
       "month         -0.0023      0.000     -5.032      0.000      -0.003      -0.001\n",
       "==============================================================================\n",
       "Omnibus:                    16624.062   Durbin-Watson:                   1.926\n",
       "Prob(Omnibus):                  0.000   Jarque-Bera (JB):            39076.087\n",
       "Skew:                           1.950   Prob(JB):                         0.00\n",
       "Kurtosis:                       4.807   Cond. No.                         15.7\n",
       "==============================================================================\n",
       "\n",
       "Warnings:\n",
       "[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.\n",
       "\"\"\""
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 값 예측하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = res.predict(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16333266233993854,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.16104236674396644,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1587520711479943,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.1564617755520222,\n",
       " 0.15417147995605007,\n",
       " ...]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted(prediction, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_binomial = [1 if p>=0.3 else 0 for p in prediction]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION 파일 이용하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9068171198939408,\n",
       " 0.9064486786279641,\n",
       " 0.9051466644158523,\n",
       " 0.8996389810845806,\n",
       " 0.896407473646308,\n",
       " 0.8952653999191234,\n",
       " 0.8951169006193903,\n",
       " 0.8949053019823713,\n",
       " 0.8947468614743204,\n",
       " 0.8947097671189543,\n",
       " 0.8934003818588635,\n",
       " 0.8933203662912034,\n",
       " 0.8921100954134039,\n",
       " 0.8914967894360755,\n",
       " 0.8910174269828216,\n",
       " 0.8900312957794704,\n",
       " 0.8897937712551934,\n",
       " 0.8891836473146604,\n",
       " 0.8890786825164142,\n",
       " 0.889010376176495,\n",
       " 0.8887785778932198,\n",
       " 0.8881560319657672,\n",
       " 0.8881093944443159,\n",
       " 0.8878558843028926,\n",
       " 0.8874638196242107,\n",
       " 0.887003698104928,\n",
       " 0.887003698104928,\n",
       " 0.8869687123121752,\n",
       " 0.8864049102185289,\n",
       " 0.8855177684424669,\n",
       " 0.8849148833068851,\n",
       " 0.8848542347264456,\n",
       " 0.8847005854454982,\n",
       " 0.8844532317759597,\n",
       " 0.8843671794060972,\n",
       " 0.8843445593217321,\n",
       " 0.8839772421638609,\n",
       " 0.8838072440143023,\n",
       " 0.8837181847542089,\n",
       " 0.883634056513132,\n",
       " 0.8836087207269014,\n",
       " 0.8835478570339992,\n",
       " 0.8834827572190563,\n",
       " 0.8832519826404156,\n",
       " 0.8828740314161991,\n",
       " 0.8828539560338912,\n",
       " 0.8824183525777645,\n",
       " 0.8824183525777645,\n",
       " 0.8820947566101489,\n",
       " 0.8816026565771535,\n",
       " 0.8815726695439522,\n",
       " 0.8814630958961103,\n",
       " 0.8814140401398286,\n",
       " 0.8813623323560982,\n",
       " 0.8811345862605595,\n",
       " 0.8806437313480742,\n",
       " 0.8805427568679605,\n",
       " 0.8805166434120809,\n",
       " 0.88033980289522,\n",
       " 0.8802145037281616,\n",
       " 0.8799301426681118,\n",
       " 0.8798282536799248,\n",
       " 0.8794851512208957,\n",
       " 0.8791650975131434,\n",
       " 0.8790572913591,\n",
       " 0.8788019751011386,\n",
       " 0.8788019751011386,\n",
       " 0.878260716845328,\n",
       " 0.8781737813109814,\n",
       " 0.8778214061931291,\n",
       " 0.8777365053741651,\n",
       " 0.8774598270901545,\n",
       " 0.8773853525150066,\n",
       " 0.877257991661337,\n",
       " 0.8769140165503082,\n",
       " 0.8768790605912351,\n",
       " 0.8766147434042972,\n",
       " 0.8764730562467481,\n",
       " 0.8760600272760954,\n",
       " 0.8760381402580969,\n",
       " 0.8759473060033652,\n",
       " 0.8758986234621075,\n",
       " 0.8756998635474719,\n",
       " 0.8756863528573807,\n",
       " 0.8755355143846775,\n",
       " 0.8751954521678496,\n",
       " 0.8751414169615731,\n",
       " 0.8750793024960473,\n",
       " 0.8748925966217621,\n",
       " 0.8748465215631114,\n",
       " 0.874751771147223,\n",
       " 0.8745565578219621,\n",
       " 0.8740129307854823,\n",
       " 0.8739556819205598,\n",
       " 0.8738086026479347,\n",
       " 0.8737487001524015,\n",
       " 0.8734919373410227,\n",
       " 0.8734319062991412,\n",
       " 0.873201873861943,\n",
       " 0.8730211484909356,\n",
       " 0.8729828657416069,\n",
       " 0.8727413563866896,\n",
       " 0.8723753448349648,\n",
       " 0.8723420933592929,\n",
       " 0.8722074487304233,\n",
       " 0.8721412784041664,\n",
       " 0.8721354518589758,\n",
       " 0.8719562424436986,\n",
       " 0.8714304013436791,\n",
       " 0.8713751840049333,\n",
       " 0.871317329985355,\n",
       " 0.8710424110315267,\n",
       " 0.8710062417647799,\n",
       " 0.8708952673951461,\n",
       " 0.8705166335345035,\n",
       " 0.8704245643159847,\n",
       " 0.870181760219401,\n",
       " 0.8700752481969356,\n",
       " 0.8700651400897391,\n",
       " 0.8699186127429529,\n",
       " 0.8698274689165298,\n",
       " 0.8698208972353726,\n",
       " 0.8697536756980968,\n",
       " 0.8697477565989612,\n",
       " 0.8696385071325279,\n",
       " 0.8695771314547642,\n",
       " 0.8694646897929806,\n",
       " 0.8693188783245367,\n",
       " 0.869248361638035,\n",
       " 0.8692371182386764,\n",
       " 0.8691497954389243,\n",
       " 0.8690489336809221,\n",
       " 0.8689078323313864,\n",
       " 0.8688629661997371,\n",
       " 0.8688150034928878,\n",
       " 0.8686823107266277,\n",
       " 0.8686260910817238,\n",
       " 0.8685497163427649,\n",
       " 0.8684677602511174,\n",
       " 0.8684648821345722,\n",
       " 0.8682981682516457,\n",
       " 0.8682702039050332,\n",
       " 0.868159477340615,\n",
       " 0.8680635264316126,\n",
       " 0.8680151027241951,\n",
       " 0.8679811002156228,\n",
       " 0.8679726516988829,\n",
       " 0.8679048338846819,\n",
       " 0.8678821503153882,\n",
       " 0.8677967375322838,\n",
       " 0.8677796066904332,\n",
       " 0.8677602228911806,\n",
       " 0.8676433463477939,\n",
       " 0.867575598814855,\n",
       " 0.8674815429629668,\n",
       " 0.8673876444016938,\n",
       " 0.8673055024688524,\n",
       " 0.8672941175078043,\n",
       " 0.8672912176345126,\n",
       " 0.8672653304101906,\n",
       " 0.8672427679037161,\n",
       " 0.8672081686058846,\n",
       " 0.8671232437924341,\n",
       " 0.8670029853491411,\n",
       " 0.8669978180247537,\n",
       " 0.8669720967970769,\n",
       " 0.8669635936062692,\n",
       " 0.8668380402829966,\n",
       " 0.8667634804500121,\n",
       " 0.8667435410350641,\n",
       " 0.8667238138845728,\n",
       " 0.8666720626696071,\n",
       " 0.866666669597904,\n",
       " 0.8663373107098095,\n",
       " 0.8662975368385077,\n",
       " 0.8662885664147129,\n",
       " 0.8662759365334419,\n",
       " 0.8662512685507059,\n",
       " 0.8660367132768524,\n",
       " 0.8660308670113924,\n",
       " 0.865981812449543,\n",
       " 0.8658899413253136,\n",
       " 0.8658153648645855,\n",
       " 0.865749106493475,\n",
       " 0.8657206846042488,\n",
       " 0.8656976840085099,\n",
       " 0.8656286622494389,\n",
       " 0.8655420183159643,\n",
       " 0.8654733619723136,\n",
       " 0.8654473916840681,\n",
       " 0.8653807632467445,\n",
       " 0.8653608656907789,\n",
       " 0.865320629730246,\n",
       " 0.8653120373766583,\n",
       " 0.8652801668631037,\n",
       " 0.8652308801983148,\n",
       " 0.8652168404159133,\n",
       " 0.8651071032866277,\n",
       " 0.8650812903394351,\n",
       " 0.8650781326053376,\n",
       " 0.8648703882791882,\n",
       " 0.8648515383120887,\n",
       " 0.8648039535553725,\n",
       " 0.8647894414068157,\n",
       " 0.8647808204598321,\n",
       " 0.8647690352117038,\n",
       " 0.8647633597188482,\n",
       " 0.8646998283271403,\n",
       " 0.864693715283857,\n",
       " 0.86468803712453,\n",
       " 0.8646210914858479,\n",
       " 0.8643347481930553,\n",
       " 0.8643199755241788,\n",
       " 0.8642997276791883,\n",
       " 0.8639312132194458,\n",
       " 0.8638963235713081,\n",
       " 0.8638958863092251,\n",
       " 0.8638817293310739,\n",
       " 0.8638730596190574,\n",
       " 0.8638406834799958,\n",
       " 0.8638381574097728,\n",
       " 0.8638205941119044,\n",
       " 0.8638114836293898,\n",
       " 0.8637710763383343,\n",
       " 0.8637071522615378,\n",
       " 0.8636603475901707,\n",
       " 0.863610999764516,\n",
       " 0.8635556996371078,\n",
       " 0.8633366248632439,\n",
       " 0.8632929060647164,\n",
       " 0.8632903714402578,\n",
       " 0.8632549044860798,\n",
       " 0.8631121793296451,\n",
       " 0.8630537311743787,\n",
       " 0.863021630693507,\n",
       " 0.863021410880332,\n",
       " 0.8630012198045844,\n",
       " 0.862998240941724,\n",
       " 0.8629835656746394,\n",
       " 0.8629310316731837,\n",
       " 0.8628899646343564,\n",
       " 0.8628690972063258,\n",
       " 0.8628546303426751,\n",
       " 0.8627611753938476,\n",
       " 0.8627173017806957,\n",
       " 0.8627113336176985,\n",
       " 0.8626587118500745,\n",
       " 0.8626322844382834,\n",
       " 0.8625732231681097,\n",
       " 0.8625559635520952,\n",
       " 0.8625297399967554,\n",
       " 0.8624271312142786,\n",
       " 0.8623182547914131,\n",
       " 0.8622655064134926,\n",
       " 0.8622597425859048,\n",
       " 0.8621334506704741,\n",
       " 0.862086412938215,\n",
       " 0.8620654427631301,\n",
       " 0.8619304488329704,\n",
       " 0.8618538004564633,\n",
       " 0.8618332434107406,\n",
       " 0.8617949010352998,\n",
       " 0.8617507680150698,\n",
       " 0.8617003965127933,\n",
       " 0.8616973938980449,\n",
       " 0.8615826885185441,\n",
       " 0.8615826885185441,\n",
       " 0.8615560872852155,\n",
       " 0.8615352712399124,\n",
       " 0.861505656250214,\n",
       " 0.8613966117707721,\n",
       " 0.8613463540621962,\n",
       " 0.8613407800329553,\n",
       " 0.8613403358784265,\n",
       " 0.8613136955665691,\n",
       " 0.8612225930235172,\n",
       " 0.8611866758786362,\n",
       " 0.8611718377777834,\n",
       " 0.861157220670277,\n",
       " 0.8611245248087351,\n",
       " 0.8610627953025992,\n",
       " 0.8610154511691809,\n",
       " 0.8609653001406979,\n",
       " 0.8609618389702214,\n",
       " 0.8609179281511065,\n",
       " 0.8608610438549571,\n",
       " 0.8608403629337147,\n",
       " 0.8608229208006161,\n",
       " 0.8608052540389243,\n",
       " 0.8607371425576371,\n",
       " 0.8607045867051195,\n",
       " 0.8606599388181472,\n",
       " 0.8606334119614005,\n",
       " 0.8605794531577563,\n",
       " 0.8605293934546104,\n",
       " 0.860333494983677,\n",
       " 0.8602903371256272,\n",
       " 0.8602738289495238,\n",
       " 0.8602530749161988,\n",
       " 0.860231871172031,\n",
       " 0.8602234537578729,\n",
       " 0.8601579108381502,\n",
       " 0.8601105321791701,\n",
       " 0.8600152868285534,\n",
       " 0.8599555069828957,\n",
       " 0.859937748029291,\n",
       " 0.859916728218633,\n",
       " 0.8599165042316819,\n",
       " 0.8598601732792147,\n",
       " 0.8598124860605231,\n",
       " 0.859741153990004,\n",
       " 0.8596901694969616,\n",
       " 0.8596068476374229,\n",
       " 0.8595710294052862,\n",
       " 0.8595590882957292,\n",
       " 0.8594426741100036,\n",
       " 0.8593733306679325,\n",
       " 0.8593648700018939,\n",
       " 0.8592959440941703,\n",
       " 0.8592752934168266,\n",
       " 0.859260059528594,\n",
       " 0.8592032828841429,\n",
       " 0.859176524090933,\n",
       " 0.8591760741372706,\n",
       " 0.8591133951898229,\n",
       " 0.8590593945217088,\n",
       " 0.859026285517827,\n",
       " 0.8589877420989805,\n",
       " 0.8589247677785541,\n",
       " 0.8589214894514375,\n",
       " 0.858765588458241,\n",
       " 0.858742042031423,\n",
       " 0.8587239288981703,\n",
       " 0.8586368437411112,\n",
       " 0.858603877553047,\n",
       " 0.8585918677328755,\n",
       " 0.8585440457519689,\n",
       " 0.8584988192039947,\n",
       " 0.8584773930760939,\n",
       " 0.8584448477403333,\n",
       " 0.8584175165787672,\n",
       " 0.8583875703978774,\n",
       " 0.8583724822995438,\n",
       " 0.8582852162560473,\n",
       " 0.8582823778584502,\n",
       " 0.858258311765731,\n",
       " 0.8582462774399627,\n",
       " 0.8581561059745566,\n",
       " 0.858041563369804,\n",
       " 0.8580008838838951,\n",
       " 0.8579030537983491,\n",
       " 0.8578999823352463,\n",
       " 0.8578907676135747,\n",
       " 0.8578574295063407,\n",
       " 0.8578458202838379,\n",
       " 0.8578422942784409,\n",
       " 0.8578079403747428,\n",
       " 0.8577414521045079,\n",
       " 0.8577010013512723,\n",
       " 0.8576614746411161,\n",
       " 0.8576375476255347,\n",
       " 0.8576164668254466,\n",
       " 0.8575920793244226,\n",
       " 0.8575861522270827,\n",
       " 0.8575799978198578,\n",
       " 0.8575283579234756,\n",
       " 0.8575255068831957,\n",
       " 0.8574743050029633,\n",
       " 0.8574681465655952,\n",
       " 0.8574472721312676,\n",
       " 0.8573894308397348,\n",
       " 0.8573018872447403,\n",
       " 0.8571957947897302,\n",
       " 0.8571808284753155,\n",
       " 0.8571658608537466,\n",
       " 0.85716277610907,\n",
       " 0.8570986668785082,\n",
       " 0.8570839198456285,\n",
       " 0.8570173773097631,\n",
       " 0.8569386821296396,\n",
       " 0.8568202295398014,\n",
       " 0.8568109564862199,\n",
       " 0.8567833624868717,\n",
       " 0.8567657246483078,\n",
       " 0.8567564486548603,\n",
       " 0.8567533565458717,\n",
       " 0.8566716750844625,\n",
       " 0.8566714467736527,\n",
       " 0.8566411929571215,\n",
       " 0.8565956880569304,\n",
       " 0.8565923646428724,\n",
       " 0.8565804409407706,\n",
       " 0.856543750963152,\n",
       " 0.8565404265470278,\n",
       " 0.8564467110546871,\n",
       " 0.8564130908794322,\n",
       " 0.8563887604644418,\n",
       " 0.8563796928486659,\n",
       " 0.856331247929218,\n",
       " 0.8563035769890867,\n",
       " 0.8563033481902788,\n",
       " 0.8562916333039183,\n",
       " 0.8562398334543612,\n",
       " 0.8562334017280799,\n",
       " 0.8562154786103274,\n",
       " 0.8561242318857667,\n",
       " 0.8561025064653283,\n",
       " 0.8559839363337033,\n",
       " 0.8559442420523613,\n",
       " 0.8559291666168732,\n",
       " 0.8559138605547015,\n",
       " 0.8559045386720691,\n",
       " 0.8558511461587225,\n",
       " 0.8557765041258877,\n",
       " 0.8557012963820921,\n",
       " 0.855663245218985,\n",
       " 0.8556421509506059,\n",
       " 0.8555321482523869,\n",
       " 0.8555139226194167,\n",
       " 0.8554710055437451,\n",
       " 0.855391602876492,\n",
       " 0.8553762493534538,\n",
       " 0.8553548901781175,\n",
       " 0.8552077316877891,\n",
       " 0.855161848710269,\n",
       " 0.8551313301326433,\n",
       " 0.8551003454341086,\n",
       " 0.8550854270468659,\n",
       " 0.8550789524147291,\n",
       " 0.8550575567722505,\n",
       " 0.8550363889814104,\n",
       " 0.8550363889814104,\n",
       " 0.8549996013893282,\n",
       " 0.8549897693064119,\n",
       " 0.8548400989003219,\n",
       " 0.8548057046876933,\n",
       " 0.8548046807211928,\n",
       " 0.8547387429174842,\n",
       " 0.8547197415114067,\n",
       " 0.8546616740122015,\n",
       " 0.8545912951998897,\n",
       " 0.8545512822962096,\n",
       " 0.8544837607570347,\n",
       " 0.8544651941334651,\n",
       " 0.854375466301865,\n",
       " 0.8543484082754524,\n",
       " 0.8543298273261808,\n",
       " 0.8543172846522566,\n",
       " 0.8542772092047917,\n",
       " 0.8542745356406115,\n",
       " 0.8542129513753374,\n",
       " 0.8541851735156004,\n",
       " 0.8541759904568335,\n",
       " 0.8541726205125125,\n",
       " 0.8541726205125125,\n",
       " 0.8541482067571191,\n",
       " 0.8541387901095946,\n",
       " 0.854104953198893,\n",
       " 0.8540742499259503,\n",
       " 0.8540433095118025,\n",
       " 0.8540184141744231,\n",
       " 0.853966398451527,\n",
       " 0.8539291537798681,\n",
       " 0.8539170464070026,\n",
       " 0.8539121784068328,\n",
       " 0.8538519608238262,\n",
       " 0.8538183008426293,\n",
       " 0.8537594703753855,\n",
       " 0.8537410593386922,\n",
       " 0.8536916441880306,\n",
       " 0.8536880327163991,\n",
       " 0.8536233272634838,\n",
       " 0.8536230949265866,\n",
       " 0.8536015211459398,\n",
       " 0.8535988373094578,\n",
       " 0.8535217322071257,\n",
       " 0.853487544190093,\n",
       " 0.8534193809321173,\n",
       " 0.8533820207010807,\n",
       " 0.8533664903898113,\n",
       " 0.8533577299961562,\n",
       " 0.8533127841046867,\n",
       " 0.8532904741277452,\n",
       " 0.8532489742886307,\n",
       " 0.8532118109728674,\n",
       " 0.8531967315292911,\n",
       " 0.8531532443140261,\n",
       " 0.8530939592154233,\n",
       " 0.8530757118689593,\n",
       " 0.8529983786995797,\n",
       " 0.8529146904101188,\n",
       " 0.8528867077851054,\n",
       " 0.8528088254860077,\n",
       " 0.8527035981598715,\n",
       " 0.8526663205809426,\n",
       " 0.8526553210380974,\n",
       " 0.8525356707130424,\n",
       " 0.8524841796773451,\n",
       " 0.8524361527596365,\n",
       " 0.8524268785817211,\n",
       " 0.8523893091375022,\n",
       " 0.8523770961961037,\n",
       " 0.8523705214500044,\n",
       " 0.8523085057137246,\n",
       " 0.8523055682082366,\n",
       " 0.8522867717093404,\n",
       " 0.8522398888865214,\n",
       " 0.8522244927185817,\n",
       " 0.8521744196616041,\n",
       " 0.8521436147228157,\n",
       " 0.852131150684931,\n",
       " 0.8521091607864201,\n",
       " 0.8520624657614642,\n",
       " 0.8520436436946786,\n",
       " 0.8519966971055748,\n",
       " 0.8519939889499906,\n",
       " 0.8518660258865561,\n",
       " 0.8518376437333883,\n",
       " 0.8518285731695464,\n",
       " 0.8518160871924247,\n",
       " 0.8518156177771542,\n",
       " 0.8518036003461943,\n",
       " 0.8518033656223757,\n",
       " 0.8517813354569906,\n",
       " 0.8517693157296157,\n",
       " 0.851744334657232,\n",
       " 0.8516157234013594,\n",
       " 0.8516066416182015,\n",
       " 0.8515630003610666,\n",
       " 0.8515318551295121,\n",
       " 0.8515191134115885,\n",
       " 0.8515159278410434,\n",
       " 0.8514943326529497,\n",
       " 0.8514786371900221,\n",
       " 0.8514563319926176,\n",
       " 0.851422216272825,\n",
       " 0.8513817182470652,\n",
       " 0.8513316447777438,\n",
       " 0.851315935122848,\n",
       " 0.8513093213597418,\n",
       " 0.851303178110255,\n",
       " 0.8513002240978789,\n",
       " 0.8512064001825274,\n",
       " 0.8511498939257696,\n",
       " 0.8511405530251841,\n",
       " 0.8511403174347125,\n",
       " 0.8511088633903298,\n",
       " 0.851005591977461,\n",
       " 0.850993284563997,\n",
       " 0.8509900896862594,\n",
       " 0.8509898538993483,\n",
       " 0.8509834639660111,\n",
       " 0.8509743502574124,\n",
       " 0.8509305555985252,\n",
       " 0.850930319733912,\n",
       " 0.8509268878735363,\n",
       " 0.8508990649958253,\n",
       " 0.8508897111123078,\n",
       " 0.8508897111123078,\n",
       " 0.850889475194393,\n",
       " 0.8508815630501647,\n",
       " 0.8508552520901974,\n",
       " 0.8508395009152968,\n",
       " 0.850830143950099,\n",
       " 0.8508109565271064,\n",
       " 0.8507983998181559,\n",
       " 0.8507828797493573,\n",
       " 0.8507828797493573,\n",
       " 0.8507356032002614,\n",
       " 0.8507200777454316,\n",
       " 0.8507171141805049,\n",
       " 0.8506981509377727,\n",
       " 0.8506919868660545,\n",
       " 0.8506915145140378,\n",
       " 0.8506821498946997,\n",
       " 0.8506604547109885,\n",
       " 0.8506476513250852,\n",
       " 0.8506227506377785,\n",
       " 0.8506161114510544,\n",
       " 0.8505216952813902,\n",
       " 0.8504718558649003,\n",
       " 0.8504434899881297,\n",
       " 0.8504163290839966,\n",
       " 0.8504027720624469,\n",
       " 0.8503837759117893,\n",
       " 0.8503805702631584,\n",
       " 0.8502642888176583,\n",
       " 0.850260844322228,\n",
       " 0.8502294739238769,\n",
       " 0.8502294739238769,\n",
       " 0.8501602978677503,\n",
       " 0.8501410394595433,\n",
       " 0.8501130957470259,\n",
       " 0.8501000167565197,\n",
       " 0.8500940692967914,\n",
       " 0.8500814628819306,\n",
       " 0.8500468499928009,\n",
       " 0.8500211556501015,\n",
       " 0.8500122305223062,\n",
       " 0.8499803430239408,\n",
       " 0.8499679658522589,\n",
       " 0.8499585638560914,\n",
       " 0.849949161374747,\n",
       " 0.8499360706045906,\n",
       " 0.8499333313850052,\n",
       " 0.8499269041351334,\n",
       " 0.8499048813783128,\n",
       " 0.8498227202579653,\n",
       " 0.849819742023222,\n",
       " 0.8498039007927822,\n",
       " 0.8497974689791378,\n",
       " 0.8497754305853384,\n",
       " 0.849759585507191,\n",
       " 0.8497407595291694,\n",
       " 0.84972193160933,\n",
       " 0.8496585244614155,\n",
       " 0.8496553060209407,\n",
       " 0.8496518499937049,\n",
       " 0.849648868969869,\n",
       " 0.8496297940146854,\n",
       " 0.849544526243218,\n",
       " 0.8495318817942323,\n",
       " 0.8495197118754315,\n",
       " 0.8494654246472271,\n",
       " 0.8494500282496245,\n",
       " 0.8494463305425215,\n",
       " 0.8494438217676985,\n",
       " 0.8494152954006007,\n",
       " 0.8494024041618738,\n",
       " 0.8493927351370126,\n",
       " 0.8493768568484482,\n",
       " 0.8493708857804645,\n",
       " 0.8493676622740152,\n",
       " 0.8493579914141676,\n",
       " 0.8493299275669385,\n",
       " 0.8492949339317998,\n",
       " 0.8491208183532201,\n",
       " 0.8491160569932326,\n",
       " 0.849076575595973,\n",
       " 0.8490475153809081,\n",
       " 0.8490316071652713,\n",
       " 0.8490258631871028,\n",
       " 0.8489808823338829,\n",
       " 0.8489559915331455,\n",
       " 0.8489525222883204,\n",
       " 0.8489497683060205,\n",
       " 0.8489465373902506,\n",
       " 0.848924396340003,\n",
       " 0.848870409643892,\n",
       " 0.8488609510979144,\n",
       " 0.8488415553816365,\n",
       " 0.848832095351386,\n",
       " 0.848816407091393,\n",
       " 0.8487942503642619,\n",
       " 0.8487940117201244,\n",
       " 0.8487750862821758,\n",
       " 0.8487656228331999,\n",
       " 0.8486194205614541,\n",
       " 0.8486004771689193,\n",
       " 0.8485715805635976,\n",
       " 0.8485623455965464,\n",
       " 0.8485558700909623,\n",
       " 0.8485401582793134,\n",
       " 0.848530682752435,\n",
       " 0.8484860587823887,\n",
       " 0.848485580694636,\n",
       " 0.8484411849123746,\n",
       " 0.848396778445853,\n",
       " 0.8483907754601742,\n",
       " 0.8483810530781211,\n",
       " 0.8483745712056168,\n",
       " 0.8483616067778664,\n",
       " 0.8483398737731533,\n",
       " 0.8483239043046084,\n",
       " 0.8483239043046084,\n",
       " 0.8483174204257489,\n",
       " 0.8482857155279767,\n",
       " 0.8482220500218518,\n",
       " 0.8482218106361125,\n",
       " 0.8482058309602148,\n",
       " 0.8481743463598425,\n",
       " 0.8481613678763051,\n",
       " 0.8481583625813143,\n",
       " 0.8481488674432675,\n",
       " 0.8481470864234515,\n",
       " 0.8481263908798494,\n",
       " 0.848123385016535,\n",
       " 0.8481231455030436,\n",
       " 0.8481229059892418,\n",
       " 0.8481101634265028,\n",
       " 0.8480851547943954,\n",
       " 0.8480596635912779,\n",
       " 0.8480496841701322,\n",
       " 0.8480019360415466,\n",
       " 0.8479671686178062,\n",
       " 0.8479606722222601,\n",
       " 0.8479476787477094,\n",
       " 0.8479193992098291,\n",
       " 0.8479031536060062,\n",
       " 0.8478661042291569,\n",
       " 0.8478491072458996,\n",
       " 0.8478395962385215,\n",
       " 0.8478361059477975,\n",
       " 0.8477530152505461,\n",
       " 0.8477502433203069,\n",
       " 0.8477214535904953,\n",
       " 0.8477049505849463,\n",
       " 0.8476989251228614,\n",
       " 0.8476959123184835,\n",
       " 0.8476896464886962,\n",
       " 0.8476603671352672,\n",
       " 0.8475746633938235,\n",
       " 0.8475648980796342,\n",
       " 0.8475330641587238,\n",
       " 0.8475250058300648,\n",
       " 0.8475202810051657,\n",
       " 0.8475107531396526,\n",
       " 0.8474977281296383,\n",
       " 0.8474947120612345,\n",
       " 0.8474947120612345,\n",
       " 0.8474849425465659,\n",
       " 0.8474849425465659,\n",
       " 0.8474784292516744,\n",
       " 0.8474721560844867,\n",
       " 0.8474596091153842,\n",
       " 0.8474335510185574,\n",
       " 0.847430293499824,\n",
       " 0.8474240187092626,\n",
       " 0.8473507611051861,\n",
       " 0.847337965420106,\n",
       " 0.8472769934514236,\n",
       " 0.8472403003382786,\n",
       " 0.847203196577671,\n",
       " 0.8471743220518998,\n",
       " 0.8471524495419899,\n",
       " 0.8471522087732566,\n",
       " 0.8471233263508322,\n",
       " 0.8471107558636501,\n",
       " 0.8471074926962345,\n",
       " 0.8471042294717424,\n",
       " 0.8471042294717424,\n",
       " 0.8471037478094681,\n",
       " 0.8470946802991121,\n",
       " 0.8470914168505227,\n",
       " 0.8470851306377425,\n",
       " 0.847075339619661,\n",
       " 0.847049468628184,\n",
       " 0.8470399166530045,\n",
       " 0.8470173055529103,\n",
       " 0.8469825945366091,\n",
       " 0.8469730391395488,\n",
       " 0.846966266922209,\n",
       " 0.8469441289777183,\n",
       " 0.846940862896507,\n",
       " 0.846918480870894,\n",
       " 0.8469117066754859,\n",
       " 0.8469086811108009,\n",
       " 0.8468895622650864,\n",
       " 0.8468699591977901,\n",
       " 0.8468636655265529,\n",
       " 0.8468586046355007,\n",
       " 0.8468443009096338,\n",
       " 0.8468221485124134,\n",
       " 0.8468191214978803,\n",
       " 0.8467834086720509,\n",
       " 0.8467801397816576,\n",
       " 0.8467738430867556,\n",
       " 0.8467612490611237,\n",
       " 0.846719467979279,\n",
       " 0.8466919940385826,\n",
       " 0.8466744068306464,\n",
       " 0.8466326063595496,\n",
       " 0.8466293348345117,\n",
       " 0.8466265461469744,\n",
       " 0.846613217816506,\n",
       " 0.846597582381029,\n",
       " 0.8465907965573993,\n",
       " 0.846584734992106,\n",
       " 0.8465363682649576,\n",
       " 0.8465333366264599,\n",
       " 0.8465295802243681,\n",
       " 0.8465043584497015,\n",
       " 0.846504116845056,\n",
       " 0.8464947786009975,\n",
       " 0.8464882306798275,\n",
       " 0.8464849566334975,\n",
       " 0.8464847150038516,\n",
       " 0.8464431138807408,\n",
       " 0.8464413164767881,\n",
       " 0.8464398390467327,\n",
       " 0.8464237057276948,\n",
       " 0.8464045373000197,\n",
       " 0.8463919184266164,\n",
       " 0.8463563658681652,\n",
       " 0.846314736278507,\n",
       " 0.846314736278507,\n",
       " 0.8462534297921239,\n",
       " 0.8462471153819102,\n",
       " 0.846237280446265,\n",
       " 0.8461761907052667,\n",
       " 0.8461696316640104,\n",
       " 0.84615347511629,\n",
       " 0.846140839365945,\n",
       " 0.8461022006545043,\n",
       " 0.8461022006545043,\n",
       " 0.8460797182782241,\n",
       " 0.8460569910347041,\n",
       " 0.8459958427004439,\n",
       " 0.845989277368802,\n",
       " 0.8459763883031904,\n",
       " 0.8459700645857978,\n",
       " 0.8459154561461355,\n",
       " 0.8459086456404248,\n",
       " 0.8458509767298971,\n",
       " 0.8458441638935762,\n",
       " 0.8458408786124368,\n",
       " 0.845802420807729,\n",
       " 0.8457573817435863,\n",
       " 0.8457477637703297,\n",
       " 0.8457148915842634,\n",
       " 0.8457090443456877,\n",
       " 0.8456814223048321,\n",
       " 0.8456250036321297,\n",
       " 0.8456186681207241,\n",
       " 0.8455799222079017,\n",
       " 0.845576389524983,\n",
       " 0.8455733423818029,\n",
       " 0.8455503111857905,\n",
       " 0.845544216009095,\n",
       " 0.8455426528455898,\n",
       " 0.8455406826585341,\n",
       " 0.8455345871712747,\n",
       " 0.8455282486270344,\n",
       " 0.8455186189756633,\n",
       " 0.8454986295066774,\n",
       " 0.845495824011826,\n",
       " 0.8454925326631133,\n",
       " 0.8454796096184302,\n",
       " 0.8454312010798044,\n",
       " 0.8454215664845932,\n",
       " 0.8454182738432132,\n",
       " 0.8454149811445217,\n",
       " 0.8453924167196251,\n",
       " 0.8453761934587991,\n",
       " 0.8453665560619811,\n",
       " 0.8453602118835892,\n",
       " 0.8453597257301569,\n",
       " 0.8453531382374664,\n",
       " 0.8453505736728935,\n",
       " 0.8453505736728935,\n",
       " 0.8453434996665502,\n",
       " 0.8452760159263839,\n",
       " 0.8452567304397917,\n",
       " 0.8452275552813285,\n",
       " 0.8452275552813285,\n",
       " 0.8451757853775166,\n",
       " 0.8451737325370142,\n",
       " 0.8451305967952731,\n",
       " 0.8451110531082126,\n",
       " 0.845107511663775,\n",
       " 0.8451047003679546,\n",
       " 0.8450912639853706,\n",
       " 0.8450882090232548,\n",
       " 0.8450816120838817,\n",
       " 0.8450625502661929,\n",
       " 0.8450559524344776,\n",
       " 0.8450559524344776,\n",
       " 0.8450397003393809,\n",
       " 0.8450366445468418,\n",
       " 0.8449746780378754,\n",
       " 0.8449718647733925,\n",
       " 0.8449622067973052,\n",
       " 0.8449441349551557,\n",
       " 0.8449329862967593,\n",
       " 0.8449118538719749,\n",
       " 0.8448971579473212,\n",
       " 0.8448889857999947,\n",
       " 0.8448874961714911,\n",
       " 0.8448870088046279,\n",
       " 0.8448712296506109,\n",
       " 0.8448613228396756,\n",
       " 0.8448486004216342,\n",
       " 0.8448063934277039,\n",
       " 0.8447995428954286,\n",
       " 0.8447773927797251,\n",
       " 0.8447707850354702,\n",
       " 0.8447547523776398,\n",
       " 0.8447445956289474,\n",
       " 0.8447318653977011,\n",
       " 0.8447219513224393,\n",
       " 0.8446898764219681,\n",
       " 0.8446766546226768,\n",
       " 0.8446410219368145,\n",
       " 0.8446407779375271,\n",
       " 0.8446244903016001,\n",
       " 0.844617877245581,\n",
       " 0.8446020757492098,\n",
       " 0.8445529546161807,\n",
       " 0.8445340838532126,\n",
       " 0.844533839716887,\n",
       " 0.8444951157933386,\n",
       " 0.8444884982447468,\n",
       " 0.8444816362631966,\n",
       " 0.8444620257528324,\n",
       " 0.8444592048976857,\n",
       " 0.8444360381088457,\n",
       " 0.8444202215267039,\n",
       " 0.8443975386242355,\n",
       " 0.8443970500014368,\n",
       " 0.8443521647263933,\n",
       " 0.84433916386126,\n",
       " 0.844329229332324,\n",
       " 0.8443096031468952,\n",
       " 0.844267032075612,\n",
       " 0.8442578272636323,\n",
       " 0.8442381937368816,\n",
       " 0.8442318117950368,\n",
       " 0.8442218716787773,\n",
       " 0.8442216271426036,\n",
       " 0.8442188027318466,\n",
       " 0.8441797702263834,\n",
       " 0.8441731417343794,\n",
       " 0.8441700720046357,\n",
       " 0.8441146983293601,\n",
       " 0.8440659410518102,\n",
       " 0.844036093261823,\n",
       " 0.8440332661086284,\n",
       " 0.8440330213310844,\n",
       " 0.8440233156633066,\n",
       " 0.8440072200598079,\n",
       " 0.8439975130832036,\n",
       " 0.8439878056142167,\n",
       " 0.8439778528047147,\n",
       " 0.843961753377119,\n",
       " 0.843951799212907,\n",
       " 0.8439191047748102,\n",
       " 0.8439058294841705,\n",
       " 0.8438945406016283,\n",
       " 0.8438930431874121,\n",
       " 0.8438705426213325,\n",
       " 0.843824798429286,\n",
       " 0.8438117616628819,\n",
       " 0.8438115166020337,\n",
       " 0.8438020447744057,\n",
       " 0.8437854403754563,\n",
       " 0.843772646042757,\n",
       " 0.843769324768734,\n",
       " 0.8437399210191477,\n",
       " 0.8437235564129261,\n",
       " 0.8437171572884999,\n",
       " 0.8436841775713018,\n",
       " 0.8436749446877653,\n",
       " 0.8436682986977057,\n",
       " 0.8436680534534656,\n",
       " 0.8436516827154708,\n",
       " 0.843645526466733,\n",
       " 0.8436422029910332,\n",
       " 0.8436355558669815,\n",
       " 0.8436094557078619,\n",
       " 0.843573623535687,\n",
       " 0.8435705441841664,\n",
       " 0.8435672194101757,\n",
       " 0.8435536743482039,\n",
       " 0.8435375392653023,\n",
       " 0.8435149969601511,\n",
       " 0.8434686769223625,\n",
       " 0.8434458810091038,\n",
       " 0.8434399636966478,\n",
       " 0.8434266552141063,\n",
       " 0.8434197550658981,\n",
       " 0.8433772314317817,\n",
       " 0.8433728134963432,\n",
       " 0.8433677381562737,\n",
       " 0.8433582444122277,\n",
       " 0.843348258894419,\n",
       " 0.843309294453195,\n",
       " 0.8433090487503677,\n",
       " 0.8433059651579734,\n",
       " 0.8432959769267357,\n",
       " 0.8432924016770506,\n",
       " 0.843279820203964,\n",
       " 0.8432698306158904,\n",
       " 0.8432405961137509,\n",
       " 0.8432405961137509,\n",
       " 0.8432405961137509,\n",
       " 0.8432108655164411,\n",
       " 0.8430967060709917,\n",
       " 0.8430869529694053,\n",
       " 0.8430638657926007,\n",
       " 0.8430607782730811,\n",
       " 0.843027931874978,\n",
       " 0.8430241055816732,\n",
       " 0.8429881642556085,\n",
       " 0.8429853216403895,\n",
       " 0.8429755629046628,\n",
       " 0.8429750706469455,\n",
       " 0.8429586408491384,\n",
       " 0.8429131716321593,\n",
       " 0.8428872195908398,\n",
       " 0.8428869733494726,\n",
       " ...]"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_val = res.predict(val_X)\n",
    "sorted(prediction_val, reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_binomial_val = [1 if p>=0.9 else 0 for p in prediction_val]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RANDOM FOREST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = RandomForestClassifier(random_state=42, n_estimators=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "forest = forest.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.01556735, 0.00502622, 0.94084573, 0.03856071])"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_forest = forest.predict(val_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict_forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE\n",
    "def mse(prediction, validation):\n",
    "    return np.mean((prediction - validation) ** 2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18454624528848942"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse(prediction_binomial_val, val['fire_yes'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18440127573209625"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mse([0] * len(val['fire_yes']), val['fire_yes'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# F1 구하기 Precision/Recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.3113831089351285\n"
     ]
    }
   ],
   "source": [
    "print('f1', metrics.f1_score(prediction_binomial_val,val['fire_yes']) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1 0.17750439367311072\n"
     ]
    }
   ],
   "source": [
    "print('f1', metrics.f1_score(predict_forest,val['fire_yes']) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 한 번에 모델 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(model):\n",
    "    a = model.fit(train_X, train_y)\n",
    "    prediction = a.predict(test_X)\n",
    "    accuracy = round(model.score(train_X, train_y) * 100, 2)\n",
    "    print(\"Accuracy : \", accuracy, \"%\")\n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  87.5 %\n"
     ]
    }
   ],
   "source": [
    "log_pred = train_and_test(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# svm_pred = train_and_test(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# knn_pred_4 = train_and_test(KNeighborsClassifier(n_neighbors = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  99.94 %\n"
     ]
    }
   ],
   "source": [
    "rf_pred = train_and_test(RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_pred = train_and_test(GaussianNB())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VALIDATION 한 번에 돌리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_val(model):\n",
    "    a = model.fit(train_X, train_y)\n",
    "    prediction_val = a.predict(val_X)\n",
    "    prediction_binomial_val = [1 if p>=0.3 else 0 for p in prediction_val]\n",
    "    print('f1:', metrics.f1_score(prediction_binomial_val,val_pp['fr_yn']) )\n",
    "    return prediction_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.20277777777777778\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_val(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples,), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.383634431455898\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_val(RandomForestClassifier(n_estimators=100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.31120552310143385\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 1], dtype=int64)"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_val(KNeighborsClassifier(n_neighbors = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f1: 0.20207612456747404\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_and_val(GaussianNB())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\utils\\validation.py:724: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:193: FutureWarning: The default value of gamma will change from 'auto' to 'scale' in version 0.22 to account better for unscaled features. Set gamma explicitly to 'auto' or 'scale' to avoid this warning.\n",
      "  \"avoid this warning.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "train_and_val(SVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
